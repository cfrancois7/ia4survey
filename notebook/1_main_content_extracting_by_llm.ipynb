{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** This notebook is a sandbox, not clean at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic augmentation with LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**:\n",
    "- Zephyr-b even with quantitization is big for the PC config. **Almost 7Go for 8Go of VRAM**.  \n",
    "--> Remove all NLP or other models that access to the GPU.\n",
    "- attention to the maximum token length regarding the GPU memory. Limit between $2^{12}$ and $2^{13}$ at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_disk(Path(\"../data/163/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = Path(\"./log\")\n",
    "\n",
    "n = 0\n",
    "for log in LOG_PATH.iterdir():\n",
    "    if log.suffix == \"log\":\n",
    "        i = log.stem.split(\"_\")[-1]\n",
    "        if i > n:\n",
    "            n = i\n",
    "log_file = LOG_PATH / f\"llm_{n+1}.log\"\n",
    "logging.basicConfig(filename=log_file.resolve(), encoding=\"utf-8\", level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 115766.35it/s]\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  5.53it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 24.78it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "# Load model\n",
    "llm = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False,\n",
    "    safetensors=True,\n",
    "    max_new_tokens=2**12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context to get the result as wanted :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "PROMPT_PATH = Path('../data/prompt/')\n",
    "with open(PROMPT_PATH / '2048_extract.json', 'r') as f:\n",
    "    prompt_2048 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(content, init_query):\n",
    "    user_query = f\"\"\"text: ```{content}```\"\"\"\n",
    "    query = init_query.copy()\n",
    "    query.append(user_query)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        query, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def query_llm(content, return_token=False):\n",
    "    prompt = format_prompt(content)\n",
    "    token_input = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    n_token_input = token_input.shape[1]\n",
    "    generation_output = llm.generate(\n",
    "        token_input,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_new_tokens=2**12,\n",
    "    )\n",
    "\n",
    "    token_output = generation_output[0]\n",
    "    n_token_output = token_output.shape[0]\n",
    "    decoded_token = tokenizer.decode(token_output, skip_special_tokens=True)\n",
    "    if return_token:\n",
    "        result = decoded_token\n",
    "    else:\n",
    "        result = decoded_token.split(\"<|assistant|>\")[-1]\n",
    "    return result, n_token_input, n_token_output\n",
    "\n",
    "\n",
    "def semantic_augmentation(content):\n",
    "    prompt = format_prompt(content)\n",
    "    json_result = query_llm(prompt)\n",
    "    result = json.loads(json_result.strip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction for input < 2k tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a run cache exists\n",
    "# if yes, load it\n",
    "# if none, create it\n",
    "CACHE_PATH = Path(\"./run\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "save_path = CACHE_PATH / \"llm_analysis.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1168\n"
     ]
    }
   ],
   "source": [
    "unique_elements, unique_indices = np.unique(\n",
    "    data[\"embeddings\"], axis=0, return_index=True\n",
    ")\n",
    "print(\"Number of duplicates:\", len(data) - len(unique_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found. Size 22273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30357/30357 [20:45:55<00:00,  2.46s/it]    \n"
     ]
    }
   ],
   "source": [
    "prev_indexes = []\n",
    "generations = []\n",
    "\n",
    "if save_path.exists():\n",
    "    with open(save_path, \"rb\") as file:\n",
    "        generations = pkl.load(file)\n",
    "        prev_indexes = [x[\"id\"] for x in generations]\n",
    "        n = len(prev_indexes)\n",
    "        print(f\"Cache file found. Size {n}\")\n",
    "\n",
    "indices = [i for i in unique_indices if i not in prev_indexes]\n",
    "index_errors = []\n",
    "\n",
    "for i, item in enumerate(tqdm(data.select(indices))):\n",
    "    success = False\n",
    "    id_ = indices[i]\n",
    "    content = item[\"content\"]\n",
    "\n",
    "    prompt = format_prompt(content, init_query=prompt_2048)\n",
    "    token_input = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "    if token_input.shape[1] <= 2048:\n",
    "        try:\n",
    "            t0 = perf_counter()\n",
    "            json_results, n_input, n_output = query_llm(content)\n",
    "            dt = perf_counter() - t0\n",
    "            result = json.loads(json_results.strip())\n",
    "            success = True\n",
    "        except KeyboardInterrupt as ki:\n",
    "            raise ki\n",
    "        except:\n",
    "            index_errors.append(id_)\n",
    "\n",
    "        msg = f\"{id_};{n_input=};{n_output=};{dt};{success}\"\n",
    "        logging.debug(msg)\n",
    "\n",
    "        if success:\n",
    "            result[\"id\"] = id_\n",
    "            generations.append(result)\n",
    "\n",
    "    # save every 50 iterations\n",
    "    if (i + 1) % 50 == 0:\n",
    "        serialized_content = pkl.dumps(generations)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(serialized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- cleaner log\n",
    "- analyze the behavior of the model for > 2048 token context + answer.\n",
    "    - define how to define and treat chunk\n",
    "    - explore langchain framework to speed-up implementation.\n",
    "- analyze the impact of context vs user command\n",
    "- define more systematic evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction for input > 2k tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purposes / context**:\n",
    "- the text with more than 2k tokens are too big for the hardware/GPU\n",
    "- more long is the text more the LLM is lost in the middle.\n",
    "\n",
    "**Protocole**:\n",
    "1. select the input with more than 2k tokens (we could use not treated content)\n",
    "2. extract chunk from doc.\n",
    "    - test langchain / llamaindex framework to chunck\n",
    "3. iterative summary of ideas with a summarizing prompt, key-word + main ideas.\n",
    "    - be sure to merge / join same/similar ideas.\n",
    "4. generate the json with last prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4gov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
