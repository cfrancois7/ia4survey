{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** This notebook is a sandbox, not clean at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic augmentation with LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**:\n",
    "- Zephyr-b even with quantitization is big for the PC config. **Almost 7Go for 8Go of VRAM**.  \n",
    "--> Remove all NLP or other models that access to the GPU.\n",
    "- attention to the maximum token length regarding the GPU memory. Limit between $2^{12}$ and $2^{13}$ at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine_learning/.anaconda3/envs/ia4gov/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to clear\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_disk(Path(\"../data/163/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = Path(\"./log\")\n",
    "\n",
    "n = 0\n",
    "for log in LOG_PATH.iterdir():\n",
    "    if log.suffix == \"log\":\n",
    "        i = log.stem.split(\"_\")[-1]\n",
    "        if i > n:\n",
    "            n = i\n",
    "log_file = LOG_PATH / f\"llm_{n+1}.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 115766.35it/s]\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:03<00:00,  9.73it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 55.56it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "# Load model\n",
    "llm = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False,\n",
    "    safetensors=True,\n",
    "    max_new_tokens=2**12,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context to get the result as wanted :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\"\"\"Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?\"\"\")\n",
    "treatment = \"\"\"- create a dict / json documet with the following keys: relevant, main_content, keyword\n",
    "- reply \"true\" or \"false\" if the text answers the question. key:\"relevant\"\n",
    "- extracts the main ideas and propositions from the text into a list.\n",
    "- reformulates these main ideas and propositions with an infinitive verb and an action. key:\"main_content\"\n",
    "- determines the word, feeling, emotion or quality that summarize the content with 0 to 5 keywords. key:\"keyword\".\n",
    "\"\"\"\n",
    "system_prompt = f\"\"\"\n",
    "- Treat the text as a french analyst who works in economical, tax, financial and public policy.\n",
    "- Applies the following analysis treatment and return the responses in JSON format.\n",
    "- respect the JSON format at all cost.\n",
    "- answers with less than 3000-tokens-words.\n",
    "\n",
    "question : ```{question}```\n",
    "treatment : ```{treatment}```\n",
    "\"\"\"\n",
    "\n",
    "example_user_1 = f\"\"\"text : ```--- 1) Réforme de l'impôt sur le revenu : il faut un paiement de l'impôt par tous dés le 1er euro perçu en prenant en compte tous les revenus.\n",
    "2) Remise à plat de toutes les niches fiscales et suppression de celles inefficaces et inutiles.\n",
    "2) Suppression de la taxe d'habitation pour 100% des français et non pas 80% car si cet impôt est bête et injuste , il l'est pour l'ensemble des français.\n",
    "3) Taxation des entreprises à un taux réel avec là aussi une revue des niches, crédits d'impôts et autres réductions qui permettent à bcp d'entreprises de se soustraire à l'impôt.\n",
    "```\"\"\"\n",
    "example_assistant_1 = \"\"\"{\"relevant\":true,\n",
    "\"main_content\": [\"payer l'impôt sur le revenu dés le 1er euro perçu\", \"supprimer les niches fiscales\", \"supprimer la taxe d'habitation\", \"Taxer les entreprises à un taux réel\"],\n",
    "\"keyword\":[\"justice\", \"réforme\", \"égalité\"]}\"\"\"\n",
    "\n",
    "\n",
    "example_user_2 = \"\"\"Que tous les français devraient travailler, ça nous coûterait moins cher.\"\"\"\n",
    "example_assistant_2 = \"\"\"{\"relevant\":false,\n",
    "\"main_content\": [\"avoir tous les français au travail\"],\n",
    "\"keyword\":[\"travail\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(content):\n",
    "    user_prompt = f\"\"\"text: ```{content}```\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example_user_1},\n",
    "        {\"role\": \"assistant\", \"content\": example_assistant_1},\n",
    "        {\"role\": \"user\", \"content\": example_user_2},\n",
    "        {\"role\": \"assistant\", \"content\": example_assistant_2},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def query_llm(content, return_token=False):\n",
    "    prompt = format_prompt(content)\n",
    "    token_input = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    n_token_input = token_input.shape[1]\n",
    "    generation_output = llm.generate(\n",
    "        token_input,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_new_tokens=2**12,\n",
    "    )\n",
    "\n",
    "    token_output = generation_output[0]\n",
    "    n_token_output = token_output.shape[0]\n",
    "    decoded_token = tokenizer.decode(token_output, skip_special_tokens=True)\n",
    "    if return_token:\n",
    "        result = decoded_token\n",
    "    else:\n",
    "        result = decoded_token.split(\"<|assistant|>\")[-1]\n",
    "    return result, n_token_input, n_token_output\n",
    "\n",
    "\n",
    "def semantic_augmentation(content):\n",
    "    prompt = format_prompt(content)\n",
    "    json_result = query_llm(prompt)\n",
    "    result = json.loads(json_result.strip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a run cache exists\n",
    "# if yes, load it\n",
    "# if none, create it\n",
    "CACHE_PATH = Path(\"./run\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "save_path = CACHE_PATH / \"llm_analysis.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1168\n"
     ]
    }
   ],
   "source": [
    "unique_elements, unique_indices = np.unique(\n",
    "    data[\"embeddings\"], axis=0, return_index=True\n",
    ")\n",
    "print(\"Number of duplicates:\", len(data) - len(unique_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- cleaner log\n",
    "- analyze the behavior of the model for > 2048 token context + answer.\n",
    "    - define how to define and treat chunk\n",
    "    - explore langchain framework to speed-up implementation.\n",
    "- analyze the impact of context vs user command\n",
    "- define more systematic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found. Size 12390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 10027/40240 [7:20:57<22:08:41,  2.64s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m ki:\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ki\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     index_errors\u001b[39m.\u001b[39mappend(id_)\n",
      "\n",
      "\u001b[1;32m/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     t0 \u001b[39m=\u001b[39m perf_counter()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     json_results, n_input, n_output \u001b[39m=\u001b[39m query_llm(content)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     dt \u001b[39m=\u001b[39m perf_counter() \u001b[39m-\u001b[39m t0\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     result \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(json_results\u001b[39m.\u001b[39mstrip())\n",
      "\n",
      "\u001b[1;32m/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m token_input \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m n_token_input \u001b[39m=\u001b[39m token_input\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m generation_output \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     token_input,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m12\u001b[39;49m,\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m )\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m token_output \u001b[39m=\u001b[39m generation_output[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/machine_learning/projects/ia4gov/notebook/test_camembert_large.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m n_token_output \u001b[39m=\u001b[39m token_output\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\n",
      "File \u001b[0;32m~/.anaconda3/envs/ia4gov/lib/python3.11/site-packages/awq/models/base.py:41\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m     40\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n",
      "\u001b[0;32m---> 41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/.anaconda3/envs/ia4gov/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/.anaconda3/envs/ia4gov/lib/python3.11/site-packages/transformers/generation/utils.py:1799\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1791\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n",
      "\u001b[1;32m   1792\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n",
      "\u001b[1;32m   1793\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n",
      "\u001b[1;32m   1794\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n",
      "\u001b[1;32m   1795\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n",
      "\u001b[1;32m   1796\u001b[0m     )\n",
      "\u001b[1;32m   1798\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n",
      "\u001b[0;32m-> 1799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n",
      "\u001b[1;32m   1800\u001b[0m         input_ids,\n",
      "\u001b[1;32m   1801\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n",
      "\u001b[1;32m   1802\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n",
      "\u001b[1;32m   1803\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n",
      "\u001b[1;32m   1804\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n",
      "\u001b[1;32m   1805\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n",
      "\u001b[1;32m   1806\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n",
      "\u001b[1;32m   1807\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n",
      "\u001b[1;32m   1808\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n",
      "\u001b[1;32m   1809\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n",
      "\u001b[1;32m   1810\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n",
      "\u001b[1;32m   1811\u001b[0m     )\n",
      "\u001b[1;32m   1813\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n",
      "\u001b[1;32m   1814\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n",
      "\u001b[1;32m   1815\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n",
      "\u001b[1;32m   1816\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n",
      "\u001b[1;32m   1817\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1822\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n",
      "\u001b[1;32m   1823\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/.anaconda3/envs/ia4gov/lib/python3.11/site-packages/transformers/generation/utils.py:2932\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n",
      "\u001b[1;32m   2930\u001b[0m \u001b[39m# sample\u001b[39;00m\n",
      "\u001b[1;32m   2931\u001b[0m probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;32m-> 2932\u001b[0m next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m   2934\u001b[0m \u001b[39m# finished sentences should have their next token be a padding token\u001b[39;00m\n",
      "\u001b[1;32m   2935\u001b[0m \u001b[39mif\u001b[39;00m eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_indexes = []\n",
    "generations = []\n",
    "\n",
    "if save_path.exists():\n",
    "    with open(save_path, \"rb\") as file:\n",
    "        generations = pkl.load(file)\n",
    "        prev_indexes = [x[\"id\"] for x in generations]\n",
    "        n = len(prev_indexes)\n",
    "        print(f\"Cache file found. Size {n}\")\n",
    "\n",
    "indices = [i for i in unique_indices if i not in prev_indexes]\n",
    "index_errors = []\n",
    "\n",
    "for i, item in enumerate(tqdm(data.select(indices))):\n",
    "    \n",
    "    success = False\n",
    "    id_ = indices[i]\n",
    "    content = item[\"content\"]\n",
    "\n",
    "    prompt = format_prompt(content)\n",
    "    token_input = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    \n",
    "    if token_input.shape[1] <= 2048:\n",
    "        try:\n",
    "            t0 = perf_counter()\n",
    "            json_results, n_input, n_output = query_llm(content)\n",
    "            dt = perf_counter() - t0\n",
    "            result = json.loads(json_results.strip())\n",
    "            success = True\n",
    "        except KeyboardInterrupt as ki:\n",
    "            raise ki\n",
    "        except:\n",
    "            index_errors.append(id_)\n",
    "\n",
    "        msg = f\"{id_};{n_input=};{n_output=};{dt};{success}\"\n",
    "        logging.debug(msg)\n",
    "\n",
    "        if success:\n",
    "            result[\"id\"] = id_\n",
    "            generations.append(result)\n",
    "\n",
    "    # save every 50 iterations\n",
    "    if (i + 1) % 50 == 0:\n",
    "        serialized_content = pkl.dumps(generations)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(serialized_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4gov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
